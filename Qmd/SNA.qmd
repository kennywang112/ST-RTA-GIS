---
title: "SNA"
format: html
editor: visual
---

```{r}
#| message: false
#| warning: false

source('../AnalyzeV2/MapperforSNA.R')
set.seed(123)

source('../Analyze/utils.R')
existing_cols <- intersect(names(all_features), names(col_translation))
names(all_features)[match(existing_cols, names(all_features))] <- col_translation[existing_cols]
```

## Social Network Analysis

This is to get the eigen centrality and betweenness

```{r}
adj_matrix <- Mapper$adjacency

adj_list <- lapply(1:Mapper$num_vertices, function(i) {
  neighbors <- which(adj_matrix[i, ] > 0)
  return(neighbors[neighbors != i])
})
# find total unique indices
uniq_idx <- sort(unique(unlist(Mapper$points_in_vertex, use.names = FALSE)))
length(uniq_idx) == dim(all_features)[1]

## Betweenness and Eigenvector Centrality
g <- graph_from_adjacency_matrix(Mapper$adjacency, mode = "undirected")
e_result <- eigen_centrality(g)
e_scores <- e_result$vector
b_scores <- betweenness(g, weights = NA, normalized = TRUE)
# 找橋樑
top_nodes <- sort(b_scores, decreasing = TRUE)

b_labels <- rep(0, length(b_scores))
top_10_indices <- head(order(b_scores, decreasing = TRUE), 10)
b_labels[top_10_indices] <- 1
e_labels <- rep(0, length(e_scores))
top_10_eigen_indices <- head(order(e_scores, decreasing = TRUE), 10)
e_labels[top_10_eigen_indices] <- 1
```

## Mapper algorithm plot

```{r}
MapperPlotter(
  Mapper,
  # label = e_scores,
  label = b_scores,
  original_data = filter_data,
  use_embedding=TRUE
)
MapperPlotter(
  Mapper,
  label = all_features$hotspot,
  # label = all_features$lr_feature,
  original_data = filter_data,
  avg=TRUE,
  use_embedding=FALSE
)
```

Get node features

```{r}

# MapperCorrelation(Mapper, original_data = filter_data, labels = list(all_features$bn_feature, all_features$hotspot), use_embedding = list(FALSE, FALSE))

length(g)
length(e_scores)

features <- tibble(
  size = node_sizes <- sapply(Mapper$points_in_vertex, length),
  eigen_centrality = e_scores,
  betweenness = b_scores,
  neighbors = adj_list, # neighbor vertex indices
  points_in_vertex = Mapper$points_in_vertex # original data indices in each vertex
)

all_features_grid <- cbind(all_features, grid_filter)

node_hotspot_values <- vapply(Mapper$points_in_vertex, function(idx) {
  mean(all_features$hotspot[idx], na.rm = TRUE)
}, numeric(1))

features$hotspot <- node_hotspot_values
features <- features%>%mutate(hotspott = ifelse(hotspot > 0.2, 1, 0))

filter_features <- features%>%
  mutate(
    neighbor_points = map(neighbors, function(nbr_ids) {
      # get all points in neighbor vertices
      points_list <- points_in_vertex[nbr_ids]
      all_nbr_points <- unlist(points_list)
      unique(all_nbr_points)
    }),
    # count of neighbor points
    neighbor_point_size = sapply(neighbor_points, length),
    # count of neighbor nodes
    neighbor_node_size = sapply(adj_list, length)
  )%>%
  filter(!map_lgl(neighbor_points, is.null))
```

# Model

```{r}
hotspot_threshold <- 0.5
df <- features %>% mutate(is_hotspot = hotspot > hotspot_threshold)
df <- df %>%
  # check if there's hot or safe neighbor first
  mutate(
    neighbor_hotspot_ratio = map_dbl(neighbors, function(idx) {
      if(length(idx) == 0) return(0)
      mean(df$is_hotspot[idx])}),
    has_safe_neighbor = neighbor_hotspot_ratio < 1,
    has_hot_neighbor = neighbor_hotspot_ratio > 0)%>%
  # check if itself is hot or safe & the neighbor_hotspot_ratio
  # set high betweenness threshold
  mutate(
    node_category = case_when(
      is_hotspot == TRUE & has_safe_neighbor == TRUE ~ "Hot_with_Safe_Neighbors",
      is_hotspot == FALSE & has_hot_neighbor == TRUE ~ "Cold_with_Hot_Neighbors",
      TRUE ~ "Others"
    ),
    is_high_betweenness = betweenness >= quantile(df$betweenness, 0.9)
    )%>% 
  # finally find the interesting point
  mutate(
    final_category = case_when(
      is_hotspot == TRUE & has_safe_neighbor == TRUE & is_high_betweenness == TRUE ~ "Hot_with_Safe_Neighbors_highbetweenness",
      is_hotspot == FALSE & has_hot_neighbor == TRUE & is_high_betweenness == TRUE ~ "Cold_with_Hot_Neighbors_highbetweenness",
       TRUE ~ "Others"
    )
  )

table(df$final_category)
```

```{r}
MapperPlotter(
  Mapper,
  label = df$final_category,
  original_data = filter_data,
  use_embedding=TRUE
)
```

# This is to classify using grid-based data (all_features.csv)

### This finds the maximum speed difference in grid

```{r}
# library(sf)
# roads <- st_read(dsn="../CalculatedData/pairs_annot_all_cities.shp")
# # grid_filter <- read_csv("../../ST-RTA/ComputedDataV2/Grid/grid_filter.csv")
# grid_gi <- read_csv("../../ST-RTA/ComputedDataV2/Grid/grid_giV1.csv")%>%
#   filter(num_accidents>0)%>%
#   mutate(index = row_number())
# 
# grid_sf <- st_as_sf(grid_gi, wkt = "geometry", crs = 3826)
# roads <- st_transform(roads, 3826)
# joined_data <- st_join(grid_sf, roads, join = st_intersects, left = TRUE)
# 
# result <- joined_data %>%
#   group_by(index) %>%
#   summarise(
#     max_spd_dlt = max(spd_dlt, na.rm = TRUE),
#     road_count = n(),
#     num_accidents = first(num_accidents),
#     .groups = "drop"
#   ) %>%
#   mutate(max_spd_dlt = ifelse(is.infinite(max_spd_dlt), 0, max_spd_dlt))

```

```{r}
source('../utils/model.R')

final_all_features <- cbind(all_features)#, result$max_spd_dlt)

result_data <- get_comparison_data(df, final_all_features)
result_data%>%
  group_by(group_label)%>%
  summarize(n())
```

### This part is to model Broken_Bridge & Resilient_Bridge

```{r}
#| message: false
#| warning: false
library(rpart)
library(rpart.plot)

cols_to_remove <- c(
  # "車道劃分設施-分道設施-路面邊線名稱_有"
  "Road Edge Line_Present"
)

analysis_data <- result_data%>%select(-hotspot, -cols_to_remove, -no_lane, -bn_feature, -lr_feature)
tree_model <- rpart(group_label ~ . , data = analysis_data, method = "class")

library(showtext)
showtext_auto()

png("../Layouts/tree_model.png", width = 1200, height = 600)
rpart.plot(tree_model, family = "PingFang", tweak = 1.2)
dev.off()
```

```{r}
library(caret)

pred_class <- predict(tree_model, newdata = analysis_data, type = "class")
conf_matrix <- confusionMatrix(data = pred_class, reference = as.factor(analysis_data$group_label))

print(conf_matrix)
```

## Find neighbor

-   直接去對A情況做自己和鄰居的決策樹，找出哪一步錯了

-   直接去對B情況做自己和鄰居的決策樹，找出哪一步是對的

```{r}

final_data_for_tree <- get_four_categories_data(df, final_all_features)
final_data_for_tree <- final_data_for_tree %>%
  mutate(Group_Label = recode(Group_Label,
    "Hot_with_Safe_Neighbors_highbetweenness" = "Broken",
    "Cold_with_Hot_Neighbors_highbetweenness" = "Resilient"
  ))

table(final_data_for_tree$Group_Label)
```

```{r}
all_features_mean <- final_data_for_tree %>%
  group_by(Group_Label) %>%
  summarise(across(where(is.numeric), \(x) mean(x, na.rm = TRUE))) %>%
  ungroup()
all_features_mean$Group_Label%>%unique()
comparison_table <- all_features_mean %>%
  pivot_longer(
    cols = -Group_Label,
    names_to = "Feature", 
    values_to = "Mean_Value"
  ) %>%
  pivot_wider(
    names_from = Group_Label,
    values_from = "Mean_Value"
  )

for (main_group in c("Broken", "Resilient")) {

  neighbor_group <- paste0(main_group, "_Neighbors")
  overlap_group  <- paste0(main_group, "_Overlap")

  group_order <- c(main_group, overlap_group, neighbor_group)

  plot_data <- comparison_table %>%
    filter(!Feature %in% l, Feature != 'Road Edge Line_Present') %>%
    mutate(diff_val = abs(.data[[main_group]] - .data[[neighbor_group]])) %>%
    arrange(desc(diff_val)) %>%
    head(20) %>%
    mutate(Feature = reorder(Feature, diff_val)) %>%
    pivot_longer(
      cols = -c(Feature, diff_val), 
      names_to = "Group", 
      values_to = "Value")%>%
    filter(Group %in% group_order) %>%
    mutate(Group = factor(Group, levels = group_order))

  p <- ggplot(plot_data, aes(x = reorder(Feature, Value), y = Value, fill = Group)) +
    geom_col(position = "dodge", width = 0.7) +
    coord_flip() +
    labs(title = paste(main_group, "Group"), y = "Mean Proportion", x = NULL) +
    theme_minimal()
  
  print(p)
}
```

```{r}
do_undersampling <- function(
    df, group1_name, group2_name
    ) {

  data_g1 <- df %>% filter(Group_Label == group1_name)
  data_g2 <- df %>% filter(Group_Label == group2_name)
  n_g1 <- nrow(data_g1)
  n_g2 <- nrow(data_g2)
  cat(sprintf("原始數量 -> %s: %d, %s: %d\n", group1_name, n_g1, group2_name, n_g2))

  target_n <- min(n_g1, n_g2)
  set.seed(123)

  data_g1_balanced <- data_g1 %>% sample_n(target_n)
  data_g2_balanced <- data_g2 %>% sample_n(target_n)
  balanced_df <- bind_rows(data_g1_balanced, data_g2_balanced)
  cat(sprintf("平衡後數量 -> 各 %d 筆 (總共 %d 筆)\n", target_n, nrow(balanced_df)))
  
  return(balanced_df)
}
```

```{r}
us_data_for_tree <- final_data_for_tree%>%select(-hotspot, -cols_to_remove, -no_lane, -bn_feature, -lr_feature)

bad_group_balanced <- do_undersampling(
  us_data_for_tree, 
  "Broken",# Hot_with_Safe_Neighbors_highbetweenness
  "Broken_Neighbors"
)

res_group_balanced <- do_undersampling(
  us_data_for_tree, 
  "Resilient", # Cold_with_Hot_Neighbors_highbetweenness
  "Resilient_Neighbors"
)

```

```{r}
#| message: false
#| warning: false
tree_bad_model <- rpart(Group_Label ~ . , 
                    data = bad_group_balanced, 
                    method = "class",
                    maxdepth = 5,
                    control = rpart.control(cp = 0.01),
                    parms = list(prior = c(0.5, 0.5)) 
                    )
pred_class <- predict(tree_bad_model, newdata = bad_group_balanced, type = "class")
conf_matrix <- confusionMatrix(data = pred_class, reference = as.factor(bad_group_balanced$Group_Label))

print(conf_matrix)

png("../Layouts/tree_bad_model.png", width = 1200, height = 800)
# rpart.plot(tree_bad_model, family = "PingFang", tweak = 1.5)
rpart.plot(tree_bad_model, 
           type = 4,
           extra = 106,
           under = TRUE,
           fallen.leaves = FALSE,
           box.palette = "BuGn",
           # shadow.col = "gray",
           nn = TRUE,
           tweak = 1,
           # varlen = 0, 
           # faclen = 0, 
)
dev.off()
```

```{r}
tree_res_model <- rpart(Group_Label ~ . , 
                    data = res_group_balanced, 
                    method = "class",
                    maxdepth = 5,
                    control = rpart.control(cp = 0.01),
                    parms = list(prior = c(0.5, 0.5)) 
                    )
pred_class <- predict(tree_res_model, newdata = res_group_balanced, type = "class")
conf_matrix <- confusionMatrix(data = pred_class, reference = as.factor(res_group_balanced$Group_Label))

print(conf_matrix)

png("../Layouts/tree_res_model.png", width = 1200, height = 900, res = 300)
rpart.plot(tree_res_model, 
           type = 4,
           extra = 106,
           under = TRUE,
           fallen.leaves = FALSE,
           box.palette = "BuGn",
           # shadow.col = "gray",
           nn = TRUE,
           tweak = 1.1,
           # varlen = 0, 
           # faclen = 0, 
)
dev.off()
```

```{r}
library(jsonlite)
export_data <- list(
  adjacency = Mapper$adjacency,
  num_vertices = Mapper$num_vertices,
  level_of_vertex = Mapper$level_of_vertex,
  points_in_vertex = Mapper$points_in_vertex,
  original_data = as.data.frame(all_features),
  # this is the label that already calculated for each node
  cc = df
)
write(toJSON(export_data, auto_unbox = TRUE), "~/desktop/ST-RTA-GIS/CalculatedData/Mapper/mapper_graph.json")
```
